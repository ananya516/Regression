{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the observed data.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity between X and Y\n",
        "\n",
        "Independence of observations\n",
        "\n",
        "Homoscedasticity (constant variance of residuals)\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "The coefficient m represents the slope of the regression line. It indicates the change in the dependent variable Y for a one-unit increase in the independent variable X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "The intercept c is the value of Y when X = 0. It is where the regression line crosses the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "ğ‘š\n",
        "=\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "(\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        ")\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "m=\n",
        "âˆ‘(X\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        " )\n",
        "2\n",
        "\n",
        "âˆ‘(X\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        " )(Y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "Y\n",
        "Ë‰\n",
        " )\n",
        "â€‹\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "To minimize the sum of the squares of residuals (differences between actual and predicted Y values), ensuring the best-fitting line.\n",
        "\n",
        "7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "RÂ² indicates the proportion of variance in the dependent variable Y that is explained by the independent variable X.\n",
        "\n",
        "RÂ² = 1 means perfect fit;\n",
        "\n",
        "RÂ² = 0 means no explanatory power.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "A regression model that predicts the value of a dependent variable based on two or more independent variables.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple Linear Regression: One independent variable\n",
        "\n",
        "Multiple Linear Regression: Two or more independent variables\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "No multicollinearity\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant. It can lead to inefficient estimates and invalid hypothesis testing.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Remove or combine correlated predictors\n",
        "\n",
        "Use dimensionality reduction (e.g., PCA)\n",
        "\n",
        "Apply regularization (Ridge, Lasso)\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Label encoding\n",
        "\n",
        "Ordinal encoding (for ordered categories)\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms allow modeling of combined effects of variables (e.g., X1 * X2), capturing relationships that arenâ€™t purely additive.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In Simple Regression, the intercept is the expected value of Y when X = 0.\n",
        "\n",
        "In Multiple Regression, it's the expected value of Y when all X variables are zero, which may not be meaningful if 0 is not within the range.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope determines how much Y changes with a one-unit change in X. It directly influences predictions and reflects the strength/direction of the relationship.\n",
        "\n",
        "17. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "Doesnâ€™t indicate if the model is biased\n",
        "\n",
        "Doesnâ€™t reflect model complexity\n",
        "\n",
        "Always increases with more variables (can be misleading)\n",
        "\n",
        "18. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "It suggests the coefficient estimate is unstable, and may not be statistically significant, reducing confidence in the variable's influence.\n",
        "\n",
        "19. What is polynomial regression?\n",
        "\n",
        "A regression technique where the relationship between the independent variable X and the dependent variable Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "20. When is polynomial regression used?\n",
        "When the relationship between variables is non-linear and cannot be accurately modeled with a straight line.\n",
        "\n",
        "21. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "It shows the baseline value of Y when all independent variables are zero, anchoring the model's predictions.\n",
        "\n",
        "22. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "In a residual plot, funnel-shaped spread or patterns indicate heteroscedasticity. It's important to address because it affects standard errors and test validity.\n",
        "\n",
        "23. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "It means that irrelevant variables might have been added, inflating RÂ² without truly improving model performance.\n",
        "\n",
        "24. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "To ensure equal contribution of features and improve model convergence (especially with regularization techniques).\n",
        "\n",
        "25. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression fits a straight line\n",
        "\n",
        "Polynomial regression fits a curved line (non-linear), using powers of X\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "Y=b\n",
        "0\n",
        "â€‹\n",
        " +b\n",
        "1\n",
        "â€‹\n",
        " X+b\n",
        "2\n",
        "â€‹\n",
        " X\n",
        "2\n",
        " +â‹¯+b\n",
        "n\n",
        "â€‹\n",
        " X\n",
        "n\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, it becomes multivariate polynomial regression, involving interactions and polynomial terms across multiple predictors.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "Risk of overfitting\n",
        "\n",
        "Non-interpretability with higher degrees\n",
        "\n",
        "Sensitive to outliers\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Adjusted RÂ²\n",
        "\n",
        "Cross-validation (CV)\n",
        "\n",
        "RMSE / MAE\n",
        "\n",
        "AIC / BIC\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "To see non-linear patterns, detect overfitting, and better communicate model fit.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)"
      ],
      "metadata": {
        "id": "bLyP82ezvoFQ"
      }
    }
  ]
}